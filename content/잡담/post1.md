---
title: BIG DATA IS DEAD 읽기
description: 23년도에 화제가 되었던 글 BIG DATA IS DEAD 관련하여 잡담
date: 2024-07-16
tags:
  - 잡담
category: 잡담
draft: false
aliases:
  - BIG DATA IS DEAD
---

## 들어가기에 앞서서

### `BIG DATA IS DEAD`가 뭔가요?


2023년도에 초에 작성되어 데이터 관련 직군에서 화제가 되었던 글 중 하나입니다. 해당 글은 쓴 사람은 `JORDAN TIGANI` 라고하는 Google Big Query를 처음부터 만든 개발자중 한명입니다. Big Query의 처음부터 함께한 개발자인 만큼 여러 고객사들의 문제를 접하고 해결했으며 2018년부터는 제품 관리로 넘어가면서 글로벌 기업들과 대화하며 데이터를 분석하였다고 합니다.

여러 고객사들의 사용 사례를 보면서 많은 곳들이 일부분의 데이터만 사용하거나 Big Query를 사용할 만큼의 데이터가 없는데도 사용하는 경우가 있었다고 말합니다.

심지어 최근에는 전통적인 데이터 처리 방법(RDB)들이 하드웨어의 발전과 성능의 개선을 통해서 상당 부분 따라잡았다고 합니다.

해당 하는 경험들을 통해서 Big Data 의 시대가 끝났다고 주장하는 글입니다.

---
## 내용 요약 및 의역

> BIG DATA IS DEAD 원본
> <https://motherduck.com/blog/big-data-is-dead/>


> [!note]
>  해당 저자는 DuckDB 를 개발하고 있는 MotherDuck에 속해있으면서 해당 글을 작성하였으므로 조금 더 객관적으로 글의 내용을 봐야 할 필요는 있습니다.

많은 벤더들은 공통적으로 데이터가 폭발적으로 증가할 것이며 이를 처리하기 위해서는 새로운 아이디어로 만들어진 기술이 필요하다고 말합니다. 하지만 그 데이터들을 한번에 모두 처리하는 경우는 많지 않기 대문에 다시 전통적인 데이터 처리 시스템들이 사용되고 있습니다.

예를 들어 MongoDB는 과거 몇 년 동안 좋은 성과와 성장을 이루었지만 최근에는 성장세가 둔화되고 MySQL 이나 PostgreSQL 같은 RDB의 강자들을 뛰어넘지 못하는 모습을 보여줍니다.

다만 OLAP 부분에서는 온프레미스에서 클라우드 서비스로 많은 서비스가 이전하고 있어 위에서 말한 상황과는 동일한 비교는 어렵습니다.

저는 Big Query에서 근무하며 고객들의 데이터 규모를 측정하는 작업을 정말 많이했습니다. 물론 빅데이터라고 부를 수 있을 정도의 엄청난 데이터 양을 가진 고객사도 있었지만 절대 다수의 고객사들은 1TB 미만의 데이터를 사용했습니다.


(대충 경험에 의해 데이터가 생각보다 많지 않다는 이야기)


최근의 데이터 플랫폼들은 저장소와 컴퓨팅이 단일 폼 팩터로 이루어져 있지 않습니다. 이 것은 20년간의 데이터 아키텍처에서 가장 중요한 변화라고 생각합니다. 예를 들면 데이터 크기가 컴퓨터보다 훨씬 빨라 질때 분리하여 스케일 아웃을 선택 할 수 있다는 것입니다.

대규모 컴퓨팅과 대규모 데이터를 다루는 것은 다르기 때문에 이러한 차이를 살펴보면 빅데이터 처리에 도움이 됩니다.

모든 데이터는 시간에 따라 증가합니다. 이러한 데이터들은 계속해서 쌓이면서 데이터의 저장 능력은 더욱 요구되겠지만 컴퓨팅 연산 능력은 초기와 크게 차이가 없을 수 있습니다. 물론 전체의 데이터에 대해서 새로운 분석을 시도 할 수도 있지만 대부분이 원하는 분석은 최근의 데이터를 분석하는 것이기 때문입니다.

따라서 거대한 테이블을 쿼리할 때도 굳이 많은 데이터들을 접근할 필요는 없습니다. 현대의 분석 데이터베이스들은 필요한 부분 집합만 처리 가능한 column projection 과 필요한 기간만 속한 partition과 필요하지 않는 partition을 골라내는 것이 가능합니다. 이외에도 다양한 쿼리의 IO를 줄이기 위한 방법들이 있으며 이것은 곧 시간=비용 감소로 이어집니다.

이러한 쿼리에 사용되는 데이터를 줄이려고 하는 노력은 비용을 용량 단위로 지불하지 않는 모델에서도 동일하게 비용 절감과 속도 향상의 결과를 가져옵니다.

대부분의 데이터는 정말 드물게 쿼리에 사용됩니다. 예를 들면 가장 최근 1년의 데이터가 전체 데이터의 30%에 불과할 수 있지만, 데이터 접근의 99%를 차지할 수 있습니다. 가장 최근 1개월의 데이터는 전체 데이터의 5%에 불과할 수 있지만, 데이터 접근의 80%를 차지할 수 있습니다.

빅데이터의 또다른 정의는 `데이터를 보관하는 비용이 어떤 데이터를 버릴 지 선택하는 비용보다 적을 것` 입니다. 이 말은 많은 사람들이 가지고 있는 빅데이터가 데이터를 필요로 해서 가지고 있는 것이 아닌 버리지 않고 가지고 있으려고 하기 때문이라는 것을 보여줍니다.

(접속가능한) 데이터를 보관하는 비용은 물리적인 데이터, 즉 byte 형태로 저장하는 것보다 비용이 높습니다. 일부 엄격한 규정에서는 법적 문제가 발생할 수 있으며 서비스가 오래되는 경우 필드명의 변경 같은 문제가 발생하기도 하여 일관성이 사라지고 추적하기가 힘들어집니다.

따라서 오래된 데이터를 보관하려면 해당 데이터가 왜 필요한지 이해하는 것이 중요합니다. 반복적인 쿼리가 지속해서 이루어진다면 해당 aggregate 결과를 저장하는 것이 더 저렴하게 이루어지지 않을까요? 만약을 위해서 대비한다면 정말로 필요할 가능성이 얼마나 될지, 얼마나 필요할지 같은 질문을 하는 것은 매우 중요합니다.

당신이 정말로 빅데이터를 위한 기술들이 필요한지 알아보기 위해서는 아래와 같은 질문을 해볼 수 있습니다.

- 정말로 엄청난 양의 데이터를 생성하고 있나요?
- 그렇다면, 정말로 한 번에 엄청난 양의 데이터를 사용해야 하나요?
- 그렇다면, 그 데이터가 정말로 한 대의 기계에 담기에는 너무 큰가요?
- 그렇다면, 단순히 데이터를 쌓아두는 것은 아닌지 확실한가요?
- 그렇다면, 데이터를 요약하는 것보다 저장하는 것이 더 좋은 게 확실한가요?

만약 하나라도 '아니오' 라고 답한다면 당신은 빅데이터보다는 자신의 데이터 크기에 맞는 도구를 사용하는 것이 맞을 수 있습니다. 기업들의 공포 마케팅으로 인해서 사용하는 기술이 아닌 당신에게 맞는 크기의 데이터 기술을 말입니다.

---
## 잡담


> 저는 데이터 엔지니어라는 직무를 정식으로 맡고 있지 않으니 실제 엔지니어분들의 현실과 맞지 않는 점이 다수 있을 수도 있습니다.

글에 언급된 column 기반 데이터베이스와 partition 을 통한 시계열 데이터 관리는 저자가 관여한 DuckDB와 Big Query 뿐만 아니라 최근 분석 시스템의 기본이기도 합니다.

많이 채택되고 있는 ClickHouse, pg의 citus 등 많은 데이터베이스가 사용중인 개념이기도 합니다.

하지만 해당 프로젝트들이 글이 작성되기 몇년 전부터 시작된 것을 감안한다면 데이터 저장보다는 처리가 중요하게 보는 트렌드는 이미 시작되었다고 볼 수도 있습니다. 

개인적으로는 여기서 얻어가야 하는 인사이트는 데이터 기술의 트렌드가 아니라 `데이터를 좀 더 깊게 보고 오버 엔지니어링을 경계하자` 아닌가 싶습니다.

데이터를 깊게 볼 수 있는 능력은 곧 설계 능력과 이어진다고도 생각하기 때문입니다. 

저 역시 처음에는 데이터 엔지니어링 공부할 때는 저장 기술 및 파이프라인에 중점을 두고 공부했었던 기억이 있습니다.

그러나 일을 하다보니 현재는 데이터를 어떻게 관리하고 상황에 따른 기술을 선택하여 시스템 설계를 하는 것이 제일 중요한 것이 아닌가 하고 생각합니다. 

왜 그렇게 생각하게 되었냐면 어떠한 기술을 단순히 사용하는 것은 참 쉽습니다.

하지만 서비스 확장 및 리소스를 고려하여 기술을 선택하고 현재 상황에 맞게 최적화 하는 것은 정말 어려운 일입니다.

그러나 이러한 작업은 시스템 설계의 기반이 되며 이러한 능력이 결국 데이터 엔지니어의 진로 중 하나인 데이터 아키텍처의 능력이 아닌가 싶습니다. 

다만 위에서는 리소스라는 단어 하나로 뭉겠지만 해당 리소스에는 서버 성능과 네트워크, 개발 능력, 일정 등을 고려해야한다는 뜻이기도 합니다.

현실적으로 이런 것들을 다 고려해서 하기에는 쉽지 않은 것은 알지만 능력 안에서 최대한 수용해야한다고 생각하며

이러한 능력을 키우기 위해서는 결국 프로젝트를 넓고 깊게 보는 눈이 필요한 것 같습니다.